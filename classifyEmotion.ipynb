{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>emotion</th>\n",
       "      <th>emotionID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>179</td>\n",
       "      <td>48</td>\n",
       "      <td>210</td>\n",
       "      <td>52</td>\n",
       "      <td>241</td>\n",
       "      <td>60</td>\n",
       "      <td>272</td>\n",
       "      <td>76</td>\n",
       "      <td>...</td>\n",
       "      <td>209</td>\n",
       "      <td>264</td>\n",
       "      <td>179</td>\n",
       "      <td>258</td>\n",
       "      <td>168</td>\n",
       "      <td>258</td>\n",
       "      <td>156</td>\n",
       "      <td>256</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>176</td>\n",
       "      <td>47</td>\n",
       "      <td>207</td>\n",
       "      <td>49</td>\n",
       "      <td>240</td>\n",
       "      <td>57</td>\n",
       "      <td>271</td>\n",
       "      <td>74</td>\n",
       "      <td>...</td>\n",
       "      <td>208</td>\n",
       "      <td>268</td>\n",
       "      <td>179</td>\n",
       "      <td>260</td>\n",
       "      <td>168</td>\n",
       "      <td>260</td>\n",
       "      <td>156</td>\n",
       "      <td>258</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>191</td>\n",
       "      <td>36</td>\n",
       "      <td>225</td>\n",
       "      <td>44</td>\n",
       "      <td>258</td>\n",
       "      <td>58</td>\n",
       "      <td>292</td>\n",
       "      <td>80</td>\n",
       "      <td>...</td>\n",
       "      <td>214</td>\n",
       "      <td>274</td>\n",
       "      <td>187</td>\n",
       "      <td>272</td>\n",
       "      <td>175</td>\n",
       "      <td>273</td>\n",
       "      <td>163</td>\n",
       "      <td>272</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>53</td>\n",
       "      <td>169</td>\n",
       "      <td>57</td>\n",
       "      <td>201</td>\n",
       "      <td>61</td>\n",
       "      <td>234</td>\n",
       "      <td>68</td>\n",
       "      <td>266</td>\n",
       "      <td>83</td>\n",
       "      <td>...</td>\n",
       "      <td>206</td>\n",
       "      <td>267</td>\n",
       "      <td>181</td>\n",
       "      <td>262</td>\n",
       "      <td>168</td>\n",
       "      <td>263</td>\n",
       "      <td>155</td>\n",
       "      <td>260</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>54</td>\n",
       "      <td>176</td>\n",
       "      <td>59</td>\n",
       "      <td>208</td>\n",
       "      <td>63</td>\n",
       "      <td>241</td>\n",
       "      <td>70</td>\n",
       "      <td>272</td>\n",
       "      <td>86</td>\n",
       "      <td>...</td>\n",
       "      <td>213</td>\n",
       "      <td>266</td>\n",
       "      <td>186</td>\n",
       "      <td>263</td>\n",
       "      <td>173</td>\n",
       "      <td>265</td>\n",
       "      <td>160</td>\n",
       "      <td>262</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 137 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   0    1   2    3   4    5   6    7   8    ...      126  127  \\\n",
       "0           0  45  179  48  210  52  241  60  272  76    ...      209  264   \n",
       "1           1  43  176  47  207  49  240  57  271  74    ...      208  268   \n",
       "2           2  31  191  36  225  44  258  58  292  80    ...      214  274   \n",
       "3           3  53  169  57  201  61  234  68  266  83    ...      206  267   \n",
       "4           4  54  176  59  208  63  241  70  272  86    ...      213  266   \n",
       "\n",
       "   128  129  130  131  132  133  emotion  emotionID  \n",
       "0  179  258  168  258  156  256  neutral          0  \n",
       "1  179  260  168  260  156  258  neutral          0  \n",
       "2  187  272  175  273  163  272  neutral          0  \n",
       "3  181  262  168  263  155  260  neutral          0  \n",
       "4  186  263  173  265  160  262  neutral          0  \n",
       "\n",
       "[5 rows x 137 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## read feature data file \n",
    "import pandas as pd\n",
    "\n",
    "features = pd.read_csv('features.csv') #,header=None) \n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[310   0   0   0   0   0   0   0]\n",
      " [  0  45   0   0   0   0   0   0]\n",
      " [  0   0  18   0   0   0   0   0]\n",
      " [  0   0   0  58   0   0   0   0]\n",
      " [  0   0   0   0  25   0   0   0]\n",
      " [  0   0   0   0   0  69   0   0]\n",
      " [  0   0   0   0   0   0  28   0]\n",
      " [  0   0   1   0   0   0   0  82]]\n"
     ]
    }
   ],
   "source": [
    "## First, LDA using all the samples!\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "X = features.iloc[:,0:134].as_matrix()   # feature points\n",
    "y = features['emotionID'].as_matrix()    # emotion id: 0 ~ 7\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(solver='svd')\n",
    "y_pred = lda.fit(X, y).predict(X)\n",
    "\n",
    "# error analysis by calc confusion matrix\n",
    "conf_mx=confusion_matrix(y, y_pred)\n",
    "print(conf_mx)\n",
    "\n",
    "#plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very good!!!\n",
    "Onely one suprise (7) image is misclassified to \"contempt\" (2)\n",
    "emotions = [\"neutral\", \"anger\", \"contempt\", \"disgust\", \"fear\", \"happy\", \"sadness\", \"surprise\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94968553459119498"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Next, LDA with 10-fold cross validation!\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "predicted = cross_val_predict(lda, X, y, cv=10)\n",
    "accuracy_score(y, predicted) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "## convert true emotion ID to one-hot encoded ID (emotionHotID)\n",
    "import numpy as np\n",
    "num_labels = 8\n",
    "def oneHot(emotionID):\n",
    "    emotionHotID = (np.arange(num_labels) == emotionID[:,None]).astype(np.float32)\n",
    "    return emotionHotID\n",
    "emotionHotID = oneHot(features['emotionID'].as_matrix())\n",
    "print(emotionHotID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134\n",
      "[[   0.   45.  179. ...,  168.  258.  156.]\n",
      " [   1.   43.  176. ...,  168.  260.  156.]\n",
      " [   2.   31.  191. ...,  175.  273.  163.]\n",
      " ..., \n",
      " [ 633.   38.  178. ...,  170.  322.  156.]\n",
      " [ 634.   49.  188. ...,  179.  303.  165.]\n",
      " [ 635.   49.  195. ...,  176.  314.  164.]]\n"
     ]
    }
   ],
   "source": [
    "input_size = len(X[0])\n",
    "print(input_size)\n",
    "X = X.astype(np.float32)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (508, 134) (508, 8)\n",
      "Validation: (128, 134) (128, 8)\n",
      "Testing: (128, 134) (128, 8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9f5850f780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## divide train and test sets using the stratified shuffle split\n",
    "# https://github.com/ageron/handson-ml/blob/master/02_end_to_end_machine_learning_project.ipynb\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(X, y):\n",
    "    train_dataset = X[train_index,:]\n",
    "    train_labels = emotionHotID[train_index,:] #y[train_index]\n",
    "    valid_dataset = X[test_index,:]\n",
    "    valid_labels = emotionHotID[test_index,:] #y[test_index]\n",
    "    \n",
    "    test_dataset = X[test_index,:]\n",
    "    test_labels = emotionHotID[test_index,:]  #y[test_index]    \n",
    "    \n",
    "#print(train_index) \n",
    "#print(test_index)\n",
    "#print(train_labels)\n",
    "#print(valid_labels)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(y[train_index])\n",
    "plt.hist(y[test_index])\n",
    "## it's indeed stratified\n",
    "\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 5747.574707\n",
      "Training accuracy: 3.0%\n",
      "Validation accuracy: 48.4%\n",
      "Loss at step 100: 477633.812500\n",
      "Training accuracy: 61.8%\n",
      "Validation accuracy: 53.1%\n",
      "Loss at step 200: 258892.171875\n",
      "Training accuracy: 46.5%\n",
      "Validation accuracy: 53.9%\n",
      "Loss at step 300: 109822.804688\n",
      "Training accuracy: 57.9%\n",
      "Validation accuracy: 78.1%\n",
      "Loss at step 400: 201393.296875\n",
      "Training accuracy: 49.2%\n",
      "Validation accuracy: 51.6%\n",
      "Loss at step 500: 90067.734375\n",
      "Training accuracy: 70.7%\n",
      "Validation accuracy: 60.9%\n",
      "Loss at step 600: 64709.425781\n",
      "Training accuracy: 77.4%\n",
      "Validation accuracy: 64.1%\n",
      "Loss at step 700: 93232.867188\n",
      "Training accuracy: 77.4%\n",
      "Validation accuracy: 78.9%\n",
      "Loss at step 800: 65173.234375\n",
      "Training accuracy: 72.2%\n",
      "Validation accuracy: 71.1%\n",
      "Loss at step 900: 86861.023438\n",
      "Training accuracy: 71.9%\n",
      "Validation accuracy: 70.3%\n",
      "Loss at step 1000: 34116.492188\n",
      "Training accuracy: 74.2%\n",
      "Validation accuracy: 64.1%\n",
      "Loss at step 1100: 86114.562500\n",
      "Training accuracy: 78.3%\n",
      "Validation accuracy: 73.4%\n",
      "Loss at step 1200: 49481.550781\n",
      "Training accuracy: 72.4%\n",
      "Validation accuracy: 70.3%\n",
      "Loss at step 1300: 50605.089844\n",
      "Training accuracy: 78.3%\n",
      "Validation accuracy: 79.7%\n",
      "Loss at step 1400: 27558.064453\n",
      "Training accuracy: 77.0%\n",
      "Validation accuracy: 82.0%\n",
      "Loss at step 1500: 18277.898438\n",
      "Training accuracy: 82.9%\n",
      "Validation accuracy: 75.8%\n",
      "Loss at step 1600: 69482.179688\n",
      "Training accuracy: 78.9%\n",
      "Validation accuracy: 78.9%\n",
      "Loss at step 1700: 35754.617188\n",
      "Training accuracy: 89.2%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 1800: 16549.097656\n",
      "Training accuracy: 87.4%\n",
      "Validation accuracy: 78.1%\n",
      "Loss at step 1900: 33324.843750\n",
      "Training accuracy: 77.0%\n",
      "Validation accuracy: 81.2%\n",
      "Loss at step 2000: 44112.765625\n",
      "Training accuracy: 77.2%\n",
      "Validation accuracy: 64.1%\n",
      "Loss at step 2100: 18343.878906\n",
      "Training accuracy: 87.8%\n",
      "Validation accuracy: 88.3%\n",
      "Loss at step 2200: 12179.165039\n",
      "Training accuracy: 85.6%\n",
      "Validation accuracy: 81.2%\n",
      "Loss at step 2300: 23703.617188\n",
      "Training accuracy: 81.3%\n",
      "Validation accuracy: 87.5%\n",
      "Loss at step 2400: 6870.346191\n",
      "Training accuracy: 85.2%\n",
      "Validation accuracy: 78.9%\n",
      "Loss at step 2500: 8615.866211\n",
      "Training accuracy: 90.6%\n",
      "Validation accuracy: 87.5%\n",
      "Loss at step 2600: 4357.576172\n",
      "Training accuracy: 93.5%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 2700: 22706.853516\n",
      "Training accuracy: 87.6%\n",
      "Validation accuracy: 85.2%\n",
      "Loss at step 2800: 15606.775391\n",
      "Training accuracy: 90.7%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2900: 1157.508423\n",
      "Training accuracy: 96.5%\n",
      "Validation accuracy: 96.9%\n",
      "Loss at step 3000: 2584.200928\n",
      "Training accuracy: 93.7%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 3100: 8505.048828\n",
      "Training accuracy: 85.8%\n",
      "Validation accuracy: 83.6%\n",
      "Loss at step 3200: 9522.771484\n",
      "Training accuracy: 89.8%\n",
      "Validation accuracy: 77.3%\n",
      "Loss at step 3300: 43655.687500\n",
      "Training accuracy: 87.2%\n",
      "Validation accuracy: 84.4%\n",
      "Loss at step 3400: 700.194214\n",
      "Training accuracy: 96.9%\n",
      "Validation accuracy: 92.2%\n",
      "Loss at step 3500: 20747.945312\n",
      "Training accuracy: 84.6%\n",
      "Validation accuracy: 86.7%\n",
      "Loss at step 3600: 27360.083984\n",
      "Training accuracy: 82.9%\n",
      "Validation accuracy: 91.4%\n",
      "Loss at step 3700: 17801.748047\n",
      "Training accuracy: 89.4%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 3800: 18380.193359\n",
      "Training accuracy: 84.1%\n",
      "Validation accuracy: 86.7%\n",
      "Loss at step 3900: 33065.335938\n",
      "Training accuracy: 87.8%\n",
      "Validation accuracy: 82.8%\n",
      "Loss at step 4000: 15015.498047\n",
      "Training accuracy: 81.3%\n",
      "Validation accuracy: 85.9%\n",
      "Loss at step 4100: 242.538849\n",
      "Training accuracy: 98.2%\n",
      "Validation accuracy: 95.3%\n",
      "Loss at step 4200: 18711.363281\n",
      "Training accuracy: 84.1%\n",
      "Validation accuracy: 72.7%\n",
      "Loss at step 4300: 7361.736816\n",
      "Training accuracy: 89.4%\n",
      "Validation accuracy: 85.9%\n",
      "Loss at step 4400: 14387.834961\n",
      "Training accuracy: 91.9%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 4500: 494.984283\n",
      "Training accuracy: 97.8%\n",
      "Validation accuracy: 95.3%\n",
      "Loss at step 4600: 907.321899\n",
      "Training accuracy: 97.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 4700: 181.878052\n",
      "Training accuracy: 98.8%\n",
      "Validation accuracy: 98.4%\n",
      "Loss at step 4800: 206.486588\n",
      "Training accuracy: 98.6%\n",
      "Validation accuracy: 98.4%\n",
      "Loss at step 4900: 151.807159\n",
      "Training accuracy: 98.8%\n",
      "Validation accuracy: 95.3%\n",
      "Loss at step 5000: 51.762165\n",
      "Training accuracy: 99.2%\n",
      "Validation accuracy: 96.9%\n",
      "Loss at step 5100: 220.980865\n",
      "Training accuracy: 98.4%\n",
      "Validation accuracy: 95.3%\n",
      "Loss at step 5200: 40.920414\n",
      "Training accuracy: 99.2%\n",
      "Validation accuracy: 95.3%\n",
      "Loss at step 5300: 35.512688\n",
      "Training accuracy: 99.2%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 5400: 100.690712\n",
      "Training accuracy: 99.2%\n",
      "Validation accuracy: 98.4%\n",
      "Loss at step 5500: 110.650284\n",
      "Training accuracy: 99.0%\n",
      "Validation accuracy: 95.3%\n",
      "Loss at step 5600: 103.713417\n",
      "Training accuracy: 99.2%\n",
      "Validation accuracy: 98.4%\n",
      "Loss at step 5700: 38.435715\n",
      "Training accuracy: 99.6%\n",
      "Validation accuracy: 98.4%\n",
      "Loss at step 5800: 100.019005\n",
      "Training accuracy: 99.2%\n",
      "Validation accuracy: 95.3%\n",
      "Loss at step 5900: 88.953773\n",
      "Training accuracy: 99.2%\n",
      "Validation accuracy: 98.4%\n",
      "Loss at step 6000: 90.796906\n",
      "Training accuracy: 99.4%\n",
      "Validation accuracy: 98.4%\n",
      "Loss at step 6100: 17.626753\n",
      "Training accuracy: 99.6%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 6200: 24.887180\n",
      "Training accuracy: 99.4%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 6300: 65.140068\n",
      "Training accuracy: 99.2%\n",
      "Validation accuracy: 96.1%\n",
      "Loss at step 6400: 71.475746\n",
      "Training accuracy: 99.2%\n",
      "Validation accuracy: 98.4%\n",
      "Loss at step 6500: 18.062099\n",
      "Training accuracy: 99.6%\n",
      "Validation accuracy: 96.1%\n",
      "Loss at step 6600: 33.479424\n",
      "Training accuracy: 99.6%\n",
      "Validation accuracy: 96.1%\n",
      "Loss at step 6700: 49.734898\n",
      "Training accuracy: 99.4%\n",
      "Validation accuracy: 96.1%\n",
      "Loss at step 6800: 51.058285\n",
      "Training accuracy: 99.0%\n",
      "Validation accuracy: 95.3%\n",
      "Loss at step 6900: 39.143608\n",
      "Training accuracy: 99.4%\n",
      "Validation accuracy: 95.3%\n",
      "Loss at step 7000: 97.396317\n",
      "Training accuracy: 98.8%\n",
      "Validation accuracy: 96.1%\n",
      "Loss at step 7100: 35.345966\n",
      "Training accuracy: 99.4%\n",
      "Validation accuracy: 96.1%\n",
      "Loss at step 7200: 49.031742\n",
      "Training accuracy: 99.4%\n",
      "Validation accuracy: 96.1%\n",
      "Loss at step 7300: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 7400: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 7500: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 7600: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 7700: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 7800: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 7900: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 8000: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 8100: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 8200: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 8300: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 8400: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 8500: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 8600: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 8700: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 8800: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 8900: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 9000: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 9100: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 9200: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 9300: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 9400: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 9500: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 9600: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 9700: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 9800: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 9900: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 10000: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 10100: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 10200: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 10300: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 10400: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 10500: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 10600: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 10700: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 10800: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 10900: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 11000: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 11100: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 11200: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 11300: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 11400: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 11500: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 11600: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 11700: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 11800: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 11900: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 12000: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 12100: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 12200: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 12300: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 12400: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 12500: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 12600: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 12700: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 12800: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 12900: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 13000: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 13100: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 13200: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 13300: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 13400: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 13500: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 13600: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 13700: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 13800: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 13900: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 14000: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 14100: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 14200: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 14300: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 14400: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 14500: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 14600: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 14700: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 14800: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 14900: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Loss at step 15000: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "Test accuracy: 97.7%\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "## Single layer, fully connected \n",
    "##\n",
    "## accuracy: 97.7%\n",
    "##\n",
    "##################################################\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  # Load the training, validation and test data into constants that are\n",
    "  # attached to the graph.\n",
    "  tf_train_dataset = tf.constant(train_dataset)\n",
    "  tf_train_labels = tf.constant(train_labels)\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  # These are the parameters that we are going to be training. The weight\n",
    "  # matrix will be initialized using random values following a (truncated)\n",
    "  # normal distribution. The biases get initialized to zero.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([input_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "  # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "  # it's very common, and it can be optimized). We take the average of this\n",
    "  # cross-entropy across all training examples: that's our loss.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using gradient descent.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  # These are not part of training, but merely here so that we can report\n",
    "  # accuracy figures as we train.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n",
    "\n",
    "\n",
    "num_steps = 15001\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # This is a one-time operation which ensures the parameters get initialized as\n",
    "  # we described in the graph: random weights for the matrix, zeros for the\n",
    "  # biases. \n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "    # and get the loss value and the training predictions returned as numpy\n",
    "    # arrays.\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "    if (step % 100 == 0):\n",
    "      print('Loss at step %d: %f' % (step, l))\n",
    "      print('Training accuracy: %.1f%%' % accuracy(\n",
    "        predictions, train_labels))\n",
    "      # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "      # just to get that one numpy array. Note that it recomputes all its graph\n",
    "      # dependencies.\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize all variables\n",
      "start iterations\n",
      "step 0:\n",
      "  minibatch loss: 16108.234375\n",
      "  minibatch accuracy: 5.9%\n",
      "  validation accuracy: 48.4%\n",
      "step 500:\n",
      "  minibatch loss: 2809760.500000\n",
      "  minibatch accuracy: 44.5%\n",
      "  validation accuracy: 48.4%\n",
      "step 1000:\n",
      "  minibatch loss: 1703993.750000\n",
      "  minibatch accuracy: 43.4%\n",
      "  validation accuracy: 48.4%\n",
      "step 1500:\n",
      "  minibatch loss: 1033395.562500\n",
      "  minibatch accuracy: 44.9%\n",
      "  validation accuracy: 48.4%\n",
      "step 2000:\n",
      "  minibatch loss: 626708.250000\n",
      "  minibatch accuracy: 45.7%\n",
      "  validation accuracy: 48.4%\n",
      "step 2500:\n",
      "  minibatch loss: 380070.875000\n",
      "  minibatch accuracy: 46.9%\n",
      "  validation accuracy: 48.4%\n",
      "step 3000:\n",
      "  minibatch loss: 230496.484375\n",
      "  minibatch accuracy: 47.3%\n",
      "  validation accuracy: 48.4%\n",
      "step 3500:\n",
      "  minibatch loss: 139786.359375\n",
      "  minibatch accuracy: 47.7%\n",
      "  validation accuracy: 48.4%\n",
      "step 4000:\n",
      "  minibatch loss: 84774.757812\n",
      "  minibatch accuracy: 46.9%\n",
      "  validation accuracy: 48.4%\n",
      "step 4500:\n",
      "  minibatch loss: 51412.738281\n",
      "  minibatch accuracy: 46.5%\n",
      "  validation accuracy: 48.4%\n",
      "step 5000:\n",
      "  minibatch loss: 31180.158203\n",
      "  minibatch accuracy: 47.3%\n",
      "  validation accuracy: 48.4%\n",
      "step 5500:\n",
      "  minibatch loss: 18909.994141\n",
      "  minibatch accuracy: 47.7%\n",
      "  validation accuracy: 48.4%\n",
      "step 6000:\n",
      "  minibatch loss: 11468.700195\n",
      "  minibatch accuracy: 48.8%\n",
      "  validation accuracy: 48.4%\n",
      "step 6500:\n",
      "  minibatch loss: 6955.876953\n",
      "  minibatch accuracy: 50.4%\n",
      "  validation accuracy: 48.4%\n",
      "step 7000:\n",
      "  minibatch loss: 4219.052246\n",
      "  minibatch accuracy: 51.2%\n",
      "  validation accuracy: 48.4%\n",
      "step 7500:\n",
      "  minibatch loss: 2559.306885\n",
      "  minibatch accuracy: 50.8%\n",
      "  validation accuracy: 48.4%\n",
      "step 8000:\n",
      "  minibatch loss: 1552.795288\n",
      "  minibatch accuracy: 45.3%\n",
      "  validation accuracy: 48.4%\n",
      "step 8500:\n",
      "  minibatch loss: 942.381226\n",
      "  minibatch accuracy: 44.5%\n",
      "  validation accuracy: 48.4%\n",
      "step 9000:\n",
      "  minibatch loss: 572.206665\n",
      "  minibatch accuracy: 43.4%\n",
      "  validation accuracy: 48.4%\n",
      "test accuracy: 48.4%\n"
     ]
    }
   ],
   "source": [
    "## two layer model with ReLU in the hidden layer\n",
    "##\n",
    "## accuracy: 48.4%\n",
    "##\n",
    "## THIS IS NOT GOOD... BECAUSE LACK OF DATA???\n",
    "###\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 256\n",
    "hidden_size = 72\n",
    "num_steps=9001\n",
    "\n",
    "reg = 1e-2\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, input_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    W1 = tf.Variable(tf.truncated_normal([input_size, hidden_size]))\n",
    "    b1 = tf.Variable(tf.zeros([hidden_size]))\n",
    "    W2 = tf.Variable(tf.truncated_normal([hidden_size, num_labels]))\n",
    "    b2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    logits = tf.matmul(tf.nn.relu(tf.matmul(tf_train_dataset, W1) + b1), W2) + b2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels,logits=logits))\n",
    "    loss += reg * (tf.nn.l2_loss(W1) + tf.nn.l2_loss(W2))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, W1) + b1), W2) + b2)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, W1) + b1), W2) + b2)\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    print(\"initialize all variables\")\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    print(\"start iterations\")\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "        if (step % 500 == 0):\n",
    "            print(\"step %d:\" % step)\n",
    "            print(\"  minibatch loss: %f\" % l)\n",
    "            print(\"  minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"  validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "            \n",
    "            #print(test_prediction.eval())\n",
    "            #print(test_labels)\n",
    "      \n",
    "\n",
    "    print(\"test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-a219ffda978e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_prediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m     \"\"\"\n\u001b[0;32m--> 570\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_dup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   4439\u001b[0m     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4440\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msession\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4441\u001b[0;31m       raise ValueError(\"Cannot evaluate tensor using `eval()`: No default \"\n\u001b[0m\u001b[1;32m   4442\u001b[0m                        \u001b[0;34m\"session is registered. Use `with \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4443\u001b[0m                        \u001b[0;34m\"sess.as_default()` or pass an explicit session to \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 544 and 128 for 'MatMul' (op: 'MatMul') with input shapes: [16,544], [128,64].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/tf3/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    685\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[1;32m    687\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf3/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimensions must be equal, but are 544 and 128 for 'MatMul' (op: 'MatMul') with input shapes: [16,544], [128,64].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-61cbebf62785>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0;31m# Training computation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m   \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_train_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m   loss = tf.reduce_mean(\n\u001b[1;32m     51\u001b[0m     tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
      "\u001b[0;32m<ipython-input-57-61cbebf62785>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mreshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer3_weights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlayer3_biases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer4_weights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlayer4_biases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf3/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   1889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m       return gen_math_ops._mat_mul(\n\u001b[0;32m-> 1891\u001b[0;31m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   1892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf3/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m_mat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   2435\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m   2436\u001b[0m         \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranspose_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2437\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   2438\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2439\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2956\u001b[0m         op_def=op_def)\n\u001b[1;32m   2957\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2958\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2959\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2207\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2209\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2210\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2211\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m~/tf3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2158\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2159\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2161\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf3/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, require_shape_fn)\u001b[0m\n\u001b[1;32m    625\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    626\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m                                   require_shape_fn)\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf3/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    689\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 544 and 128 for 'MatMul' (op: 'MatMul') with input shapes: [16,544], [128,64]."
     ]
    }
   ],
   "source": [
    "## CNN  (NEED TO MATCH DIMENSIONS)\n",
    "\n",
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "num_channels=1\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, input_size, 1, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [input_size // 4 * 1 // 4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))\n",
    "\n",
    "\n",
    "\n",
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shape' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-8ae9a22b4037>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'shape' is not defined"
     ]
    }
   ],
   "source": [
    "shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
